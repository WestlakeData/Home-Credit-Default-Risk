---
title: "Home Credit Default Risk"
subtitle: "EDA"
author: "Chris Porter"
output:
  html_document:
    df_print: paged
---

```{r libraries, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(skimr)
library(ggplot2)
library(gridExtra)
library(grid)

```

```{r load data, echo=FALSE, warning=FALSE}
data <- read.csv('../data/application_train.csv')
```
## Missing Data
The dataset is highly dimensional and has 121 independent variables listed.  It is important to understand the degree of completeness of these variables in order to determine their usefulness in the model building phase of the project.
```{r skimr analysis}
skm <- skim(data)
# Create DF showing Columns with missing data
skm_missing <- select(skm, skim_variable, n_missing, complete_rate) %>% filter(n_missing > 0) %>% arrange(complete_rate)
skm_missing
```
Of the 121 independent variables, it turns out half (61 rows) have missing data.  Before determining what to do about each of these variables (removal, imputation, etc.) it is important to examine the columns and have a better understanding of what each of these columns may contribute in terms of useful information towards solving the eventual question of whether or not the customer is likely to default on the loan.

### Character Columns
```{r missing character}
skm_char <- skm %>% filter(skim_type == 'character') %>% select(skim_variable,character.min,character.max,character.empty) %>% filter(character.empty > 0)
skm_char
```
Six of the 16 character variables contain missing information.  Of those 6 columns, only 2 contain information that is informative towards the likelihood of customer default, NAME_TYPE_SUITE (NA = 1292) and OCCUPATION_TYPE (NA=96391).  In both of these cases it is impossible know whether the NA signifies the lack of a value (i.e. unhoused or unemployed) of if the information is just incomplete.  Therefore it is proposed to fill all NAs in both of this columns with a new value 'unknown'.  The other 4 columns will be dropped in light of the large number of missing values, and the small amount of variance in the values, in addition to the lack of relevance to the target variable outcome.

### Numeric Housing Data
Of the 61 variables that have missing data, 47 of these variables describe the housing of applicant in significant detail.  While an understanding of the housing situation is likely to be informative of the customer's default risk, the detail provided in these columns is probably not necessary, much of the important information is included in NAME_HOUSING_TYPE and FLAG_OWN_REALTY which are both complete.  It may be informative to disaggregate those that live in a house as opposed to an apartment, and we can look further into this possibility. However, inclusion of these 47 variables in the final model is probably unnecessary and so missing values in these columns can be ignored. 

```{r non-housing missing variables, echo=FALSE, include=FALSE}
housing_stats_cols <- c('APARTMENTS_AVG','BASEMENTAREA_AVG','YEARS_BEGINEXPLUATATION_AVG','YEARS_BUILD_AVG','COMMONAREA_AVG',
                        'ELEVATORS_AVG','ENTRANCES_AVG','FLOORSMAX_AVG','FLOORSMIN_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG',
                        'LIVINGAREA_AVG','NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_AVG','APARTMENTS_MODE','BASEMENTAREA_MODE',
                        'YEARS_BEGINEXPLUATATION_MODE','YEARS_BUILD_MODE','COMMONAREA_MODE','ELEVATORS_MODE','ENTRANCES_MODE',
                        'FLOORSMAX_MODE','FLOORSMIN_MODE','LANDAREA_MODE','LIVINGAPARTMENTS_MODE','LIVINGAREA_MODE',
                        'NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_MODE','APARTMENTS_MEDI','BASEMENTAREA_MEDI',
                        'YEARS_BEGINEXPLUATATION_MEDI','YEARS_BUILD_MEDI','COMMONAREA_MEDI','ELEVATORS_MEDI','ENTRANCES_MEDI',
                        'FLOORSMAX_MEDI','FLOORSMIN_MEDI','LANDAREA_MEDI','LIVINGAPARTMENTS_MEDI','LIVINGAREA_MEDI',
                        'NONLIVINGAPARTMENTS_MEDI','NONLIVINGAREA_MEDI','FONDKAPREMONT_MODE','HOUSETYPE_MODE','TOTALAREA_MODE',
                        'WALLSMATERIAL_MODE','EMERGENCYSTATE_MODE')
# Remove housing data columms
skm_missing %>% filter(!skim_variable %in% housing_stats_cols)
```
### Social Circle Data
Looking at the remaining columns with missing data 4 of them contain information about the applicant's social circle and the 30-day and 60-day days past due default status, there is minimal missing data (n=1021)

```{r social circle data, message=FALSE, warning=FALSE}
social_data_df <- data %>% select(OBS_30_CNT_SOCIAL_CIRCLE,DEF_30_CNT_SOCIAL_CIRCLE,OBS_60_CNT_SOCIAL_CIRCLE,
                                  DEF_60_CNT_SOCIAL_CIRCLE)
summary(social_data_df)

plot_OBS_30 <- ggplot(social_data_df, aes(OBS_30_CNT_SOCIAL_CIRCLE)) +
  geom_histogram()
plot_DEF_30 <- ggplot(social_data_df, aes(DEF_30_CNT_SOCIAL_CIRCLE)) +
  geom_histogram()
plot_OBS_60 <- ggplot(social_data_df, aes(OBS_60_CNT_SOCIAL_CIRCLE)) +
  geom_histogram()
plot_DEF_60 <- ggplot(social_data_df, aes(DEF_60_CNT_SOCIAL_CIRCLE)) +
  geom_histogram()

grid.arrange(plot_OBS_30,plot_DEF_30,plot_OBS_60,plot_DEF_60, ncol=2, top=textGrob('Social Circle Variable Distribution'))
```

Given that there are relatively few rows with missing data and the data is heavily right-skewed, it is proposed that missing data in these columns be replaced with imputed zero values (*median*= 0),

### External Source Data
Included in the dataset are 3 externally sourced scores which have been normalized.  There is wide variation in the number of values missing in each of these sources ($NA_1=173378$,$NA_2=660$,$NA_3=60965$).  
```{r External Source Data, echo=FALSE}
summary(data %>% select(EXT_SOURCE_1,EXT_SOURCE_2,EXT_SOURCE_3))
# Select rows with no External Source Data
n_empty_ext <- nrow(data %>% filter(is.na(EXT_SOURCE_1) & is.na(EXT_SOURCE_2) & is.na(EXT_SOURCE_3)))
```
Removing rows with missing data would shrink the dataset considerably.  Imputation with the mean value for each column is a possibility.  Another option is to create a new variable combining the scores into an AVG_EXT_SCORE since they are all normalized values.  The benefit to this approach is that only `r n_empty_ext` rows have no value for any of the 3 columns and so very little data would need to be imputed or removed.  This approach takes advantage of available information, while reducing missing data.  However it is possible that some information loss will occur with this approach if one of these sources turns out to be much more informative than one or both of the others.

### Credit Bureau Data
The final large group of related data deals with credit inquiries from the credit bureau over the previous year.  These are divided into the hour, day, week, month, quarter, and year (each time period is exclusive of the time period before it) prior to the application being submitted.  This data is strongly right-skewed and is 86.5% complete (NAs = 41519).

```{r Credit Bureau Data}
summary(data %>% select(AMT_REQ_CREDIT_BUREAU_HOUR,AMT_REQ_CREDIT_BUREAU_DAY,AMT_REQ_CREDIT_BUREAU_WEEK,AMT_REQ_CREDIT_BUREAU_MON,
                        AMT_REQ_CREDIT_BUREAU_QRT,AMT_REQ_CREDIT_BUREAU_YEAR))


```
While we could remove the 41519 rows missing data, the loss of other information in those records might be valuable, and so it is proposed to impute the missing values with the median value for each column, in most cases this is 0.

### Other Data
There are a few remaining columns that contain missing data OWN_CAR_AGE, AMT_GOODS_PRICE, AMT_ANNUITY, CNT_FAM_MEMBERS,and DAYS_LAST_PHONE_CHANGE. Of these columns only OWN_CAR_AGE has a significant number of missing values.  There is another column which is related to car ownership, FLAG_OWN_CAR, which is a boolean field.  Upon closer inspection it is evident that all of those records where FLAG_OWN_CAR = 'N' contain NAs, conversely only 5 records with a FLAG_OWN_CAR = 'Y' contain missing values.
```{r Car Ownership Data, warning=FALSE}
library(gmodels)
CrossTable(data$FLAG_OWN_CAR, is.na(data$OWN_CAR_AGE), prop.r = F, prop.c = F, prop.chisq = F, dnn = c('Owns Car','Contains Missing Values'))
```
Therefore, it is proposed to remove these 5 records, and to convert the OWN_CAR_AGE into a factor variable with levels: No Car Owned, 0-3 years, 4-6 years, etc.

The other columns contain so few observations that it is proposed that those rows with missing values be excluded.

## Problems with the data
### Outlier Detection

```{r}
skm_num <- skm %>% filter(skim_type == 'numeric') %>% filter(!skim_variable %in% housing_stats_cols) %>% select(skim_variable, numeric.p0, numeric.mean, numeric.p100, numeric.hist)
skm_num
```
Some modelling techniques are susceptible to outsized influence by outliers (regression for example).  After examining the distribution of the numeric data a few columns stand out as containing likely outliers.  The first examples are CNT_FAM_MEMBERS and CNT_CHILDREN.  
```{r family counts}
# Create dataframe with columns of concern for plotting
num_problem_df <- data %>% select(CNT_CHILDREN, CNT_FAM_MEMBERS, AMT_INCOME_TOTAL, OWN_CAR_AGE, DAYS_LAST_PHONE_CHANGE, DAYS_EMPLOYED) %>% filter(!is.na(CNT_FAM_MEMBERS))

# Create Plots for family columns
plt_fam <- ggplot(num_problem_df, aes(CNT_FAM_MEMBERS)) +
  geom_boxplot()
plt_chld <- ggplot(num_problem_df, aes(CNT_CHILDREN)) +
  geom_boxplot()

grid.arrange(plt_fam, plt_chld, ncol=2, top=textGrob('Family Count Variables Distribution'))

```
  
Both of these columns (which are related) contain a small number of outlier records, however the information is consistent and is likely to have an influence on the target variable.  As such, it is recommended that this variable is normalized before it is used in any modelling techniques that are suceptible to the presence of outliers.

### Erroneous Data

In the case of AMT_INCOME_TOTAL there is a single outlier that lies so far out of the scale (AMT_INCOME_TOTAL = 117,000,000) and is inconsistent with other data, and so is probably erroneous.  Even if not erroneous, the value added by this datapoint to any model is negligible as the core customer group is not multi-millionaires.

```{r income amount}
plt_inc <- ggplot(num_problem_df, aes(AMT_INCOME_TOTAL)) +
  geom_boxplot() +
  ggtitle('Total Income Distribution')

# data %>% filter(AMT_INCOME_TOTAL > 100000000)
plt_inc2 <- ggplot(data %>% filter(AMT_INCOME_TOTAL < 100000000), aes(AMT_INCOME_TOTAL)) +
  geom_boxplot() +
  ggtitle('Total Income Distribution (After Removal)')

grid.arrange(plt_inc,plt_inc2, ncol=2, top=textGrob('Total Income Variable Distribution'))
```
Even after removal, outliers remain and measures (see above) will have to be taken to ensure they do not overly influence predictive models.  In the case of such strongly skewed data, it might be advisable to convert the continuous variable into a categorical one and break the data into bins for use in models.

```{r car age, warning=F}
ggplot(num_problem_df, aes(OWN_CAR_AGE)) +
  geom_histogram(bins = 60) +
  ggtitle('Age of Cars Owned Distribution')

data %>% filter(OWN_CAR_AGE > 55) %>% select(SK_ID_CURR, OWN_CAR_AGE) %>% group_by('Cars older than 55 years' = as.factor(OWN_CAR_AGE))%>%count()
```
There is an odd anomaly in the data such that a large number of owned cars are recorded as being 64-65 years old.  It seems unlikely that this data correctly represents reality.  The amount of erroneous data is small and so removal wouldn't impact the overall dataset to a large degree.  However, the other data contained in these records may be informative, and so it is proposed that if the variable OWN_CAR_AGE is to be used in a model, that these rows be removed.

```{r phone, warning=FALSE, message=FALSE}
ggplot(num_problem_df, aes(DAYS_LAST_PHONE_CHANGE)) +
  geom_histogram() +
  ggtitle('Days Since Previous Phone Change')

```

```{r days employed, message=FALSE}

data %>% filter(DAYS_EMPLOYED > 0) %>% select(SK_ID_CURR, DAYS_EMPLOYED) %>% group_by('Days Employed' = as.factor(DAYS_EMPLOYED))%>%count()

ggplot(num_problem_df %>% filter(DAYS_EMPLOYED < 0), aes(DAYS_EMPLOYED)) +
  geom_histogram() +
  ggtitle('Days Employed')

de_df <- data %>% filter(DAYS_EMPLOYED < 0) 

```

Finally, DAYS_EMPLOYED has significant data integrity issues.  The way the data is set up only values less than or equal to 0 should be permissible.  However, a significant number of rows (n = 55374) all have the same positive value, DAYS_EMPLOYED = 365243.  Not only should the value be negative, but the scale is also outside of realistic values (365243 days = 1000+ years).  Due to these rows comprising a significant portion of the data set (`r round(55374/nrow(data)*100,1)`%) removal would be detrimental to model accuracy, so it is proposed that the data be imputed for these rows, being replaced by the column median (median = `r median(de_df$DAYS_EMPLOYED)`)

