---
title: "Home Credit Default Risk"
subtitle: "Modelling"
author: "Roman Brock, Che Diaz Fadel, Kalyani Joshi, and Chris Porter"
output: 
  html_document:
    number_sections: yes
    toc: yes
    fig_width: 15
    fig_height: 10
    highlight: tango
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Set options ----
options(tibble.print_max = 40,
        tibble.print_min = 24,
        width = 222,
        pillar.min_title_chars = 15)

```

# Introduction  
Unbanked individuals represent both an underserved demographic as well as a typically untapped market by reputable creditors. Home Credit seeks to fill this gap in service. There are unique challenges that accompany establishing creditworthiness among a population that by definition has little to no financial history, verifiable assets, or traditional means to qualify for a loan.

## Project Scope
This project will utilize machine learning algorithms to develop a classification model which will use available data about Home Credit customers to improve prediction of those that are likely to repay loans granted by Home Credit. The team will test a number of possible classification models in order to develop the most accurate model on data outside the training data. A successful model will provide greater perfomance in terms of limiting Type II errors, than a simple prediction based upon majority class statistics and will allow Home Credit to loan to customers with confidence that repayment will in return grow available assets to the company in order to further its mission of providing credit to the underserved.

The dataset was previously explored and was determined to be suitable for data modelling purposes.  In this notebook we document the various modeling strategies explored as well as any included data preparation undertaken prior to modeling.  We then compare the various models performance and report our results.

# Data Preparation
## Load Libraries
```{r libraries}
# xgBoost dependencies
library(tidyverse)
library(gmodels)
library(caret)
library(mlr)
library(xgboost)
library(parallel)
library(parallelMap)
library(ggplot2)
library(pROC)
```
## Standardized Data Set Preparation
In accordance with the information detailed in our previous EDA notebook, we created a standardized data set that utilized the same variables for all models.
```{r standardized data sets, echo=TRUE, eval=FALSE}
# Load data ----
app_train <- read_csv("../data/application_train.csv")

bureau_df <- read_csv("../data/bureau.csv")

desired_columns <- read_csv("desired_columns.csv")

# Identify numerically encoded categorical variables ----
num2fac <- app_train %>%
  select(where(is.numeric)) %>%
  mutate(across(everything(), as.character)) %>%
  select(where(~ all(!grepl("\\.|-", .)))) %>%
  #select(-c(own_car_age, hour_appr_process_start, matches("^(obs|def|amt|cnt)"))) %>%
  select(-c(OWN_CAR_AGE, HOUR_APPR_PROCESS_START, matches("^(OBS|DEF|AMY|CNT)"))) %>%
  colnames() 

# Create data frame ----
app_train1 <- app_train %>%
  # Select columns from 'desired_columns.csv'
  select(desired_columns$ColumnName) %>% 
  # Handling numerically encoded categorical variables
  mutate(across(c(where(is.character), all_of(num2fac)), factor), 
         # Fixing invalid NA's
         across(c(CODE_GENDER, ORGANIZATION_TYPE), 
                ~case_when(. != "XNA" ~ .)),
         # Replacing NA's in social circle columns with 0
         across(contains("SOCIAL_CIRCLE"), ~replace_na(., 0)),
         # Replacing NA's with 0
         across(c(AMT_REQ_CREDIT_BUREAU_HOUR, AMT_REQ_CREDIT_BUREAU_DAY,
                  AMT_REQ_CREDIT_BUREAU_WEEK, AMT_REQ_CREDIT_BUREAU_MON,
                  AMT_REQ_CREDIT_BUREAU_QRT, AMT_REQ_CREDIT_BUREAU_YEAR),
                ~replace_na(., factor("0"))),
         # Fixing unusual `DAYS_EMPLOYED` values
         DAYS_EMPLOYED = case_when(DAYS_EMPLOYED <= 0 ~ DAYS_EMPLOYED),
         # Creating ordinal version of `OWN_CAR_AGE`
         OWN_CAR_AGE2 = cut(OWN_CAR_AGE,
                            c(0, 5, 10, 15, Inf),
                            right = FALSE),
         OWN_CAR_AGE2 = case_when(FLAG_OWN_CAR == "N" ~ factor("No Car Owned"),
                                  .default = OWN_CAR_AGE2)) %>%
  # Creating aggregate variable from the `EXT_SOURCE_*` variables
  rowwise() %>%
  mutate(AVG_EXT_SCORE = mean(c_across(contains("EXT_"))),
         .after = EXT_SOURCE_3) %>%
  ungroup() %>%
  # Removing rows with NA's in below columns
  filter(if_all(c(FLAG_OWN_CAR, AMT_GOODS_PRICE, AMT_ANNUITY, 
                  CNT_FAM_MEMBERS, DAYS_LAST_PHONE_CHANGE),
                ~!is.na(.)))

# Aggregate bureau data ----
# __ Wide version data frame
bureau_agg_a <- bureau_df %>%
  # Seemed to be the most relevant credit types
  filter(CREDIT_TYPE %in% c("Consumer credit","Credit card","Car loan","Mortgage",
                            "Microloan","Loan for business development","Another type of loan"),
         # Ensure credit was actually given
         AMT_CREDIT_SUM > 0,
         # Thought this made most sense
         CREDIT_ACTIVE %in% c("Active", "Closed"),
         # It's unclear if they convert nominal curreny values so kept only most common
         CREDIT_CURRENCY == "currency 1") %>%
  # Clean credit type values to use as column names
  mutate(CREDIT_TYPE = gsub(" +", "_", tolower(CREDIT_TYPE))) %>%
  group_by(SK_ID_CURR, CREDIT_TYPE) %>%
  summarise(avg_credit = mean(AMT_CREDIT_SUM),
            # Decided that average of ratios made more sense than ratio of averages
            avg_overage = mean(AMT_CREDIT_MAX_OVERDUE/AMT_CREDIT_SUM, na.rm = TRUE)) %>%
  pivot_wider(names_from = CREDIT_TYPE,
              values_from = c(avg_credit, avg_overage)) %>%
  ungroup()

# __ Version in EDA ----
bureau_agg_b <- bureau_df %>%
  group_by(SK_ID_CURR) %>%
  summarise(avg_days_credit = mean(DAYS_CREDIT, na.rm = TRUE),
            avg_credit_day_overdue = mean(CREDIT_DAY_OVERDUE, na.rm = TRUE),
            avg_days_credit_enddate = mean(DAYS_CREDIT_ENDDATE, na.rm = TRUE),
            avg_amt_credit_max_overdue = mean(AMT_CREDIT_MAX_OVERDUE, na.rm = TRUE),
            avg_cnt_credit_prolong = mean(CNT_CREDIT_PROLONG, na.rm = TRUE),
            avg_amt_credit_sum = mean(AMT_CREDIT_SUM, na.rm = TRUE),
            avg_amt_credit_sum_debt = mean(AMT_CREDIT_SUM_DEBT, na.rm = TRUE),
            avg_amt_credit_sum_limit = mean(AMT_CREDIT_SUM_LIMIT, na.rm = TRUE),
            avg_amt_credit_sum_overdue = mean(AMT_CREDIT_SUM_OVERDUE, na.rm = TRUE))

#Convert bureau_agg_b$SK_ID_CURR to factor
bureau_agg_b$SK_ID_CURR <- as.factor(bureau_agg_b$SK_ID_CURR)
#Join app_train1 and bureau_agg_b
app_train2 <- app_train1 %>% inner_join(bureau_agg_b)
# Cleanup
remove(app_train,bureau_agg_a,bureau_agg_b,bureau_df,desired_columns,num2fac)

```
This resulted in two datasets app_train1 which contained only information from the application_train.csv dataset, and app_train2 which contained the same information as app_train1, but added aggregated data from bureau.csv.

In addition to this data preparation, some models required specific treatment of the data to get it into a format that was able to be utilized by that particular model.

## XGBoost
The XGBoost algorithm requires the use of a numeric matrix.  As such, categorical variables were converted into one-hot encoded dummy variables and boolean values were converted into numeric variables.  Additionally, non-informative ID variable SK_ID_CURR was removed from the matrix and saved for later analysis.
```{r xgBoost Data Preparation, eval=FALSE}
app_train1 <- readRDS('app_train1.RDS')
app_train2 <- readRDS('app_train2.RDS')

# Create Train and Test sets for Modeling
set.seed(1234)
train.index <- sample(nrow(app_train1), floor(nrow(app_train1)*0.7), replace = F)
train_df <- app_train1[train.index,]
test_df <- app_train1[-train.index,]
remove(train.index, app_train1)

#Verify Target variable distribution in Train and Test
CrossTable(train_df$TARGET)
CrossTable(test_df$TARGET)

ratio <- (sum(ifelse(train_df$TARGET == '0',1,0)))/(sum(ifelse(train_df$TARGET == '1',1,0))) # Ratio Negative Response : Positive Response

# Handle IDs
id.train <- train_df$SK_ID_CURR
id.test <- test_df$SK_ID_CURR
target.train <- ifelse(train_df$TARGET == '1',1,0)
target.test <- ifelse(test_df$TARGET == '1',1,0)
train <- train_df %>% select(-c(SK_ID_CURR,TARGET,OWN_CAR_AGE)) # Add TARGET
test <- test_df %>% select(-c(SK_ID_CURR,TARGET,OWN_CAR_AGE)) # Add TARGET

# Convert Factor Variables to Numeric (Train)
train_df <- train_df %>% select(-c(SK_ID_CURR,OWN_CAR_AGE)) # Add TARGET
test_df <- test_df %>% select(-c(SK_ID_CURR,OWN_CAR_AGE)) # Add TARGET

train_df$TARGET <- ifelse(train_df$TARGET == '1',1,0) # TARGET
train_df$FLAG_OWN_CAR <- ifelse(train_df$FLAG_OWN_CAR == 'Y',1,0) # FLAG_OWN_CAR
train_df$FLAG_OWN_REALTY <- ifelse(train_df$FLAG_OWN_REALTY == 'Y',1,0) # FLAG_OWN_REALTY
for(c in 1:length(colnames(train_df))){
  if(grepl('FLAG_',colnames(train_df)[c])){ 
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert FLAG_ Factor columns to numeric
  } else if (grepl('REG_', colnames(train_df)[c])) {
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert REG_ Factor columns to numeric
  } else if (grepl('LIVE_', colnames(train_df)[c])) {
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert LIVE_ Factor columns to numeric
  } else if (grepl('AMT_REQ_CREDIT_BUREAU_', colnames(train_df)[c])) {
    train_df[c] <- lapply(train_df[c], function(x) as.numeric(as.character(x))) # Convert Credit Bureau Inquiries Columns to numeric
  }
}

# Convert Factor Variables to Numeric (Test)
test_df$TARGET <- ifelse(test_df$TARGET == '1',1,0) # TARGET
test_df$FLAG_OWN_CAR <- ifelse(test_df$FLAG_OWN_CAR == 'Y',1,0) # FLAG_OWN_CAR
test_df$FLAG_OWN_REALTY <- ifelse(test_df$FLAG_OWN_REALTY == 'Y',1,0) # FLAG_OWN_REALTY
for(c in 1:length(colnames(test_df))){
  if(grepl('FLAG_',colnames(test_df)[c])){ 
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert FLAG_ Factor columns to numeric
  } else if (grepl('REG_', colnames(test_df)[c])) {
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert REG_ Factor columns to numeric
  } else if (grepl('LIVE_', colnames(test_df)[c])) {
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert LIVE_ Factor columns to numeric
  } else if (grepl('AMT_REQ_CREDIT_BUREAU_', colnames(test_df)[c])) {
    test_df[c] <- lapply(test_df[c], function(x) as.numeric(as.character(x))) # Convert Credit Bureau Inquiries Columns to numeric
  }
}

#Convert remaining Factor variables to dummy variables
dummy <- dummyVars(" ~ .", data=train_df)
dtrain_df <- data.frame(predict(dummy, newdata=train_df))
dummy <- dummyVars(" ~ .", data=test_df)
dtest_df <- data.frame(predict(dummy, newdata=test_df))

#Return TARGET to factor variable
dtrain_df$TARGET <- as.factor(dtrain_df$TARGET)
dtest_df$TARGET <- as.factor(dtest_df$TARGET)
```

# Modeling Process
A variety of models were prepared for evaluation

# Logistic Regression
Logistic regression used to model the relationship between a binary dependent variable "TARGET" variable all the other variables in the dtrain_df dataset.

```{r Logistic regression, echo=TRUE, eval=FALSE}
#Fit Logistic Regression Model
model <- glm(TARGET ~ ., data = dtrain_df, family = "binomial")
summary(model)
```
Predict the probabilities of the target variable for both the training and testing data using the logistic regression model (model) that was previously fitted.

```{r Predict probabilities for training and testing data, echo=TRUE, eval=FALSE}
train_probs <- predict(model, newdata = dtrain_df, type = "response")
test_probs <- predict(model, newdata = dtest_df, type = "response")
```
These probabilities represent the likelihood of each observation belonging to the positive class of the target variable.

```{r Calculate AUC scores, echo=TRUE, eval=FALSE}
#Area Under the Curve (AUC) scores for the training and testing data based on the predicted probabilities obtained from the logistic regression model.
train_roc <- roc(dtrain_df$TARGET, train_probs)
train_auc <- auc(train_roc)
test_roc <- roc(dtest_df$TARGET, test_probs)
test_auc <- auc(test_roc)

## Display AUC scores
output <- data.frame(
    Model = "Logistic Regression",
    "Training AUC" = train_auc,
    "Testing AUC" = test_auc
)
print(output)
```
Training AUC: 0.7705759, Testing AUC: 0.7463063 -->  the logistic regression model shows a moderate level of discrimination between the classes, with better performance on the training data compared to the testing data.

```{r  Plot ROC curve, echo=TRUE, eval=FALSE}
roc_obj <- roc(dtest_df$TARGET, test_probs)

# Create data frame for plotting
roc_data <- data.frame(
    FPR = roc_obj$specificities,
    TPR = roc_obj$sensitivities
)

# Calculate AUC
roc_auc <- auc(roc_obj)

# Plot Curve
ggplot(roc_data, aes(x = TPR, y = FPR)) +
    geom_line(color = "darkorange") +
    geom_abline(slope = -1, intercept = 1, linetype = "dashed", color = "navy") +
    xlim(0, 1) +
    ylim(0, 1) +
    labs(title = "Receiver Operating Characteristic",
         x = "True Positive Rate",
         y = "False Positive Rate") +
    annotate(
        "text", x = 0.6, y = 0.2,
        label = paste0("AUC = ", round(roc_auc, 2)),
        color = "darkorange"
    ) +
    theme_minimal()

```
Higher AUC value on the training data compared to the testing data. This indicates that the model performs relatively well in distinguishing between positive and negative cases, but there might be some drop in performance when applied to unseen data

```{r Getting plot for significant features, echo=TRUE, eval=FALSE}
## Get significant features
## Identify the significant features based on their p-values from logistic model.
summary_model <- summary(model)
significant_features <- summary_model$coefficients[summary_model$coefficients[, "Pr(>|z|)"] < 0.01, ]
significant_features <- format(significant_features, scientific = FALSE)

# Get significant column names
significant_columns <- rownames(significant_features)

# Check if significant column names exist in app_train2
existing_columns <- intersect(significant_columns, colnames(app_train2))

# Get rows for significant columns
significant_rows <- app_train2[, c("TARGET", existing_columns)]

# Save significant rows to a new file
write.csv(significant_rows, file = "significant_rows.csv", row.names = FALSE)

# Create display_importances function
display_importances <- function(feature_importance_df_) {
    feature_importance_df_ <- feature_importance_df_ %>%
        arrange(desc(importance))
    
    plt <- ggplot(feature_importance_df_, aes(x = importance, y = reorder(feature, importance, max))) +
        geom_bar(stat = "identity", fill = "steelblue") +
        labs(title = "Variable Importance",
             x = "Importance",
             y = "Feature") +
        theme_minimal()
    
    print(plt)
}

# Get the important feature names
important_features <- colnames(significant_rows)[-1]

# Calculate the importance values based on the number of features
importance_values <- 1:length(important_features)

# Create the feature_importance_df data frame
feature_importance_df <- data.frame(
    feature = important_features,
    importance = importance_values
)

# Call the display_importance function to get variable importance plot
display_importances(feature_importance_df)

```
Based on the analysis of the logistic regression model, the variables Flag_Document 8/3 have the most significant impact on the outcome values. These variables, along with the Ext source 3,2,1 variables from the application dataset, play an important role in predicting the outcomes.

While the exact sources of the external data (Ext source 3,2,1) are not specified by Home Credit, these variables prove to be valuable in predicting the outcomes. Therefore, it is recommended that the company collects more data from these external data sources (1, 2, and 3) to further improve the accuracy of the predictions.

In addition to the external data sources, other important variables for predicting outcomes include the flags on phone and work phone, as well as the variables related to the number of days employed and the number of days since birth. These variables also contribute significantly to the prediction of the outcomes.

Overall, based on the analysis, it is recommended to focus on gathering more data from the application dataset and the specified external data sources (1, 2, and 3) to enhance the predictive power of the model.

## XGBoost
XGBoost is a Boosted Gradient algorithm.  It has a number of user set hyperparameters that can influence model performance.  In order to determine the best hyperparameter settings for the model, a random tuning grid was utilized.  Initially, a grid with larger ranges and fewer iterations was used to evaluate if any of the hyperparameters had regions of the grid that appeared to yield better performance.  Once the grid was refined, a final tuning run was made to yield the best performing model on the data.  The gamma hyperparameter was not utilized as it is more effective with smaller trees than the model eventually utilized.

```{r XGBoost Hyperparameter Tuning, eval=FALSE}
# Set up Tasks
trainTask <- makeClassifTask(data = dtrain_df, target = 'TARGET')
testTask <- makeClassifTask(data = dtest_df, target = 'TARGET')

# one-hot encoding
trainTask <- createDummyFeatures(obj = trainTask)
testTask <- createDummyFeatures(obj = testTask)

# Create Learner
lrn <- makeLearner('classif.xgboost', predict.type = 'response')
lrn$par.vals <- list(objective = 'binary:logistic',
                     booster = 'gbtree',
                     eval_metric = 'error'
                     )
makeWeightedClassesWrapper(lrn,wcw.weight = ratio)

# Set Hyperparameter Space
params <- makeParamSet(makeIntegerParam('max_depth', lower = 8, upper = 9),
                       makeIntegerParam('nrounds', lower = 200, upper = 500),
                       makeNumericParam('min_child_weight', lower = 5, upper = 10),
                       makeNumericParam('subsample', lower = 0.5, upper = 0.8),
                       makeNumericParam('colsample_bytree', lower = 0.7, upper = 1),
                       makeNumericParam('eta', lower = 0.1, upper = 0.4))

# Set resampling
rdesc <- makeResampleDesc('CV', stratify = F, iters = 5)

# Set search
ctrl <- makeTuneControlRandom(maxit = 25)

# Set up Parallel Processing
parallelStartSocket(cpus = detectCores())

# Tuning
tune <- tuneParams(learner = lrn, 
                   task = trainTask,
                   resampling = rdesc,
                   measures = fpr,
                   par.set = params,
                   control = ctrl,
                   show.info = T)
```

### Hyperparameters
Hyperparameter tuning took several hours running in parallel on 16 cpus.  The models were scored on performance in minimizing the FPR measure, as the minimization of Type II errors was determined to be paramount to model performance based on business objectives.

The selected hyperparameters were as follows:  
max_depth = 8  
nrounds = 280  
min_child_weight = 6.96752  
subsample = 0.5409828  
colsample_bytree = 0.7388845  
eta = 0.3878024  

```{r train tuned xgBoost model, eval=FALSE}
# Select Hyperparameters from the best model
lrn.tune <- setHyperPars(lrn, par.vals = tune$x)

# Train the model using selected hyperparameters
xgb.model <- train(learner = lrn.tune,
                task = trainTask)
```
### Benchmarking
Training the model using the selected hyperparameters was benchmarked and averaged 285s per training run.

# Model Performance
```{r load saved models, echo=FALSE}
# Load Saved Models
xgb.model <- readRDS('best_model_tuned2.xgb')

# From this point forward code will be evaluated during Knitting operations ####
```

## XGBoost Model Performance
```{r hidden data prep, echo=FALSE}
app_train1 <- readRDS('app_train2.RDS')

# Create Train and Test sets for Modeling
set.seed(1234)
train.index <- sample(nrow(app_train1), floor(nrow(app_train1)*0.7), replace = F)
train_df <- app_train1[train.index,]
test_df <- app_train1[-train.index,]
remove(train.index, app_train1)
# Ratio Negative Response : Positive Response
ratio <- (sum(ifelse(train_df$TARGET == '0',1,0)))/(sum(ifelse(train_df$TARGET == '1',1,0))) 

# Handle IDs
id.train <- train_df$SK_ID_CURR
id.test <- test_df$SK_ID_CURR
target.train <- ifelse(train_df$TARGET == '1',1,0)
target.test <- ifelse(test_df$TARGET == '1',1,0)

# Convert Factor Variables to Numeric (Train)
train_df <- train_df %>% select(-c(SK_ID_CURR,OWN_CAR_AGE)) # Add TARGET
test_df <- test_df %>% select(-c(SK_ID_CURR,OWN_CAR_AGE)) # Add TARGET

train_df[77:85] <- train_df[77:85] %>% replace(is.na(.), 0) # Replace NaN values
test_df[77:85] <- test_df[77:85] %>% replace(is.na(.), 0) # Replace NaN values
train_df$TARGET <- ifelse(train_df$TARGET == '1',1,0) # TARGET
train_df$FLAG_OWN_CAR <- ifelse(train_df$FLAG_OWN_CAR == 'Y',1,0) # FLAG_OWN_CAR
train_df$FLAG_OWN_REALTY <- ifelse(train_df$FLAG_OWN_REALTY == 'Y',1,0) # FLAG_OWN_REALTY
for(c in 1:length(colnames(train_df))){
  if(grepl('FLAG_',colnames(train_df)[c])){ 
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert FLAG_ Factor columns to numeric
  } else if (grepl('REG_', colnames(train_df)[c])) {
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert REG_ Factor columns to numeric
  } else if (grepl('LIVE_', colnames(train_df)[c])) {
    train_df[,c] <- ifelse(train_df[,c] == '1',1,0) # Convert LIVE_ Factor columns to numeric
  } else if (grepl('AMT_REQ_CREDIT_BUREAU_', colnames(train_df)[c])) {
    train_df[c] <- lapply(train_df[c], function(x) as.numeric(as.character(x))) # Convert Credit Bureau Inquiries Columns to numeric
  }
}

# Convert Factor Variables to Numeric (Test)
test_df$TARGET <- ifelse(test_df$TARGET == '1',1,0) # TARGET
test_df$FLAG_OWN_CAR <- ifelse(test_df$FLAG_OWN_CAR == 'Y',1,0) # FLAG_OWN_CAR
test_df$FLAG_OWN_REALTY <- ifelse(test_df$FLAG_OWN_REALTY == 'Y',1,0) # FLAG_OWN_REALTY
for(c in 1:length(colnames(test_df))){
  if(grepl('FLAG_',colnames(test_df)[c])){ 
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert FLAG_ Factor columns to numeric
  } else if (grepl('REG_', colnames(test_df)[c])) {
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert REG_ Factor columns to numeric
  } else if (grepl('LIVE_', colnames(test_df)[c])) {
    test_df[,c] <- ifelse(test_df[,c] == '1',1,0) # Convert LIVE_ Factor columns to numeric
  } else if (grepl('AMT_REQ_CREDIT_BUREAU_', colnames(test_df)[c])) {
    test_df[c] <- lapply(test_df[c], function(x) as.numeric(as.character(x))) # Convert Credit Bureau Inquiries Columns to numeric
  }
}

#Convert remaining Factor variables to dummy variables
dummy <- dummyVars(" ~ .", data=train_df)
dtrain_df <- data.frame(predict(dummy, newdata=train_df))
dummy <- dummyVars(" ~ .", data=test_df)
dtest_df <- data.frame(predict(dummy, newdata=test_df))

#Return TARGET to factor variable
dtrain_df$TARGET <- as.factor(dtrain_df$TARGET)
dtest_df$TARGET <- as.factor(dtest_df$TARGET)

# Set up Tasks
trainTask <- makeClassifTask(data = dtrain_df, target = 'TARGET')
testTask <- makeClassifTask(data = dtest_df, target = 'TARGET')
```

XGBoost model was used to predict the classification of applications as No Default = 0 or Default = 1.  No Default (TARGET = 0) was defined as a positive prediction for the purposes of analysis.

```{r xgboost performance analysis}
# Predict classification on test split of app_train2
xgb.pred <- predict(xgb.model,testTask)
# ConfusionMatrix of prediction results
confusionMatrix(xgb.pred$data$response,xgb.pred$data$truth)
```

Two metrics were examined for model comparison based upon business objectives.  Specificity (TNR) and Recall (TPR) were computed. TPR was most informative as it is the complement of miss rate (FNR) which was the measure we sought to minimize.

```{r FPR and TNR: XGBoost}
TP <- 71398
TN <- 597
FN <- 5550
FP <- 1431

TPR <- TP / ( TP + FP )
TNR <- TN / ( TN + FN )
```

The model resulted in a Recall(TPR) = `r TPR` and Specificity(TNR) = `r TNR`.

# Results


# Contributions
* Roman Brock
    + Random Forest
* Che Diaz Fadel
    + Support Vector Machines
* Kalyani Joshi
    + Logistic Regression
* Chris Porter
    + XGBoost

